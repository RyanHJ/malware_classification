import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix
import feature_generation

import utilities
import naive_bayes_classifier
import svm_classifier
import rf_classifier

from matplotlib import rcParams
rcParams.update({'figure.autolayout': True})

def generate_confusion_matrix(target, predictions,filename):
    cm = confusion_matrix(target, predictions ,utilities.target_names)
    
    """
    print('Confusion matrix, without normalization')
    print(cm)
    plt.figure()
    plot_confusion_matrix(cm,'Confusion Matrix', filename)
    """

    # Normalize the confusion matrix by row (i.e by the number of samples
    # in each class)
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print('Normalized confusion matrix')
    print(cm_normalized)

    plot_confusion_matrix(cm_normalized, 'Normalized Confusion Matrix', filename)

    #plt.savefig('../results/filename' + '.png')
    plt.show()

def plot_confusion_matrix(cm, title,filename):
    cmap=plt.cm.Blues
    np.set_printoptions(precision=2)
    plt.figure()
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(utilities.target_names))
    plt.xticks(tick_marks, utilities.target_names, rotation=45)
    plt.yticks(tick_marks, utilities.target_names)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

    plt.savefig('../results/' + filename + '_' + title + '.png')

def plot_roc_curve(y_test, y_score):

    print y_score

    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(len(utilities.target_names)):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # Plot of a ROC curve for a specific class
    plt.figure()
    plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    # Plot ROC curve
    plt.figure()
    plt.plot(fpr["micro"], tpr["micro"],
             label='micro-average ROC curve (area = {0:0.2f})'
             ''.format(roc_auc["micro"]))
    for i in range(n_classes):
        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'
                 ''.format(i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Some extension of Receiver operating characteristic to multi-class')
    plt.legend(loc="lower right")
    plt.show()


def generate_classification_report(y_true,y_pred,filename):
    from sklearn.metrics import classification_report
    report = classification_report(y_true, y_pred, utilities.target_names)
    print report

    plot_classification_report(report, filename)
    f = open('../results/' + filename + 'classification_report.csv', 'w')
    f.write(report)
    f.close()

def plot_classification_report(cr,  filename):
    """
    Courtesy of an answer on stackexchange
    """
    title = 'Classification Report'
    with_avg_total=True
    cmap=plt.cm.Blues

    lines = cr.split('\n')

    classes = []
    plotMat = []
    for line in lines[2 : (len(lines) - 3)]:
        #print(line)
        t = line.split()
        # print(t)
        classes.append(t[0])
        v = [float(x) for x in t[1: len(t) - 1]]
        print(v)
        plotMat.append(v)

    if with_avg_total:
        aveTotal = lines[len(lines) - 1].split()
        classes.append('avg/total')
        vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]
        plotMat.append(vAveTotal)

    plt.imshow(plotMat, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    x_tick_marks = np.arange(3)
    y_tick_marks = np.arange(len(classes))
    plt.xticks(x_tick_marks, ['Precision', 'Recall', 'F1-Score'], rotation=45)
    plt.yticks(y_tick_marks, classes)
    plt.tight_layout()
    plt.ylabel('Classes')
    plt.xlabel('Measures')

    plt.savefig('../results/' + filename + '_' + title + '.png',bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    (header,data,target) = feature_generation.generate_features()
    
    (target,predictions)= naive_bayes_classifier.classify(data,target)
    generate_confusion_matrix(target,predictions,'naive_bayes')
    generate_classification_report(target,predictions, 'naive_bayes')
    
    """
    (target,predictions)= svm_classifier.classify(data,target)
    generate_confusion_matrix(target,predictions,'svm')
    generate_classification_report(target,predictions, 'svm')
    """

    (target,predictions)= rf_classifier.classify(data,target)
    generate_confusion_matrix(target,predictions,'rf')
    generate_classification_report(target,predictions, 'rf')
